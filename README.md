GPT-Style Transformer Language Model (From Scratch)

This project is an implementation of a GPT-style Transformer language model built entirely from scratch in PyTorch.
It includes:
	‚Ä¢	A Bigram Language Model for understanding token prediction basics.
	‚Ä¢	A Transformer-based GPT model with multi-head self-attention, feedforward layers, and positional embeddings.
	‚Ä¢	Full training loop with evaluation and text generation.

‚∏ª

 Features
	‚Ä¢	Character-level tokenization for raw text input.
	‚Ä¢	Configurable hyperparameters for batch size, learning rate, layers, heads, and embeddings.
	‚Ä¢	Multi-Head Self-Attention mechanism.
	‚Ä¢	Dropout regularization for reducing overfitting.
	‚Ä¢	AdamW optimizer for stable training.
	‚Ä¢	Generates coherent text based on learned patterns.

‚∏ª

üìÇ Project Structure

  üìÅ NanoGPT-From-Scratch
‚îÇ‚îÄ‚îÄ Nano_GPT.py               # Jupyter Notebook with complete implementation
‚îÇ‚îÄ‚îÄ input.txt                  # Training dataset (Tiny Shakespeare or other text)

How It Works
	1.	Data Preprocessing:
	  ‚Ä¢	Reads raw text (input.txt).
	  ‚Ä¢	Creates character-to-integer and integer-to-character mappings.
	  ‚Ä¢	Splits dataset into train and validation sets.
	2.	Model Architecture:
	  ‚Ä¢	Token and positional embeddings.
	  ‚Ä¢	Multi-head self-attention.
	  ‚Ä¢	Feedforward layers.
	  ‚Ä¢	Layer normalization.
	  ‚Ä¢	Final linear layer projecting to vocabulary size.
	3.	Training:
	  ‚Ä¢	Uses AdamW optimizer.
	  ‚Ä¢	Evaluates loss periodically on train/val sets.
	  ‚Ä¢	Learns to predict the next token given a sequence.
	4.	Text Generation:
	  ‚Ä¢	Starts with a given prompt (or empty context).
	  ‚Ä¢	Predicts one token at a time.
	  ‚Ä¢	Samples from probability distribution for creativity.

 Running on Apple Silicon (M1/M2) GPU

PyTorch supports the Metal Performance Shaders (MPS) backend for Apple Silicon GPUs.
	1.	Install PyTorch with MPS Support
      pip install torch torchvision torchaudio
     
  2.	Enable MPS in Code
      In the notebook, set the device:
      device = 'mps' if torch.backends.mps.is_available() else 'cpu'

  3. Verify is MPS is available 
     import torch
     print(torch.backends.mps.is_available())  # Should return True

  4.	Run Training
      The model will automatically use the GPU when device='mps'.

Example Training Output
25.4 M parameters
step 0:     train loss 11.3376, val loss 11.3179
step 5000:  train loss 7.9628,  val loss 7.9739
step 10000: train loss 5.9162,  val loss 5.9487
step 19999: train loss 3.9739,  val loss 4.0487

